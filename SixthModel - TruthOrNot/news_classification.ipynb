{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W19F10pfrTI"
      },
      "source": [
        "# NLTK settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCRU18yyT216",
        "outputId": "544aadbd-1430-4d87-ee48-be1175a32648"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\roshan muhammed\n",
            "[nltk_data]     r\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\roshan muhammed\n",
            "[nltk_data]     r\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mv8h_wl2LOh8"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72emctiPOfmj"
      },
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6KjAd2fXLo2t"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"train.csv\")[['text','label']]\n",
        "dataset = shuffle(train)\n",
        "dataset = dataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "he5Oo10XNcfM"
      },
      "outputs": [],
      "source": [
        "wn = WordNetLemmatizer()\n",
        "def text_preprocessing(text):\n",
        "    try:\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = text.split()\n",
        "        text = [word for word in text if word not in stopwords.words('english')]\n",
        "        text = [wn.lemmatize(word) for word in text]\n",
        "        text = ' '.join(text)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        print(f\"Problematic text: {text}\")\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JaDQSIUGTful"
      },
      "outputs": [],
      "source": [
        "dataset['text'] = dataset['text'].apply(text_preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.to_csv(\"preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwUNKWt4o_hx"
      },
      "source": [
        "# Train - validation - test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "foP2jPplpN3K"
      },
      "outputs": [],
      "source": [
        "X = dataset['text']\n",
        "y = dataset['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXz9D2Xbo9aN",
        "outputId": "adebfe2f-3bd1-4386-de95-d117ee588e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data :  (16608,)\n",
            "Validation Data :  (3322,)\n",
            "Test Data :  (831,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_validation, y_validation, test_size=0.2, random_state=123)\n",
        "\n",
        "print('Training Data : ', X_train.shape)\n",
        "print('Validation Data : ', X_validation.shape)\n",
        "print('Test Data : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJnQ6AAOqS1o"
      },
      "source": [
        "# Feature Extraction - TFIDF - SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JHqcQbRpuXxn"
      },
      "outputs": [],
      "source": [
        "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)),\n",
        "                      ])\n",
        "\n",
        "text_clf_svm = text_clf_svm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3yyl-5Rx61q"
      },
      "source": [
        "# Validation set performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqvtWKVGvkHr",
        "outputId": "a492f03e-c30c-4a33-a147-3f184a7256b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.921733895243829\n",
            "Precision: 0.9075492341356673\n",
            "Recall: 0.948\n"
          ]
        }
      ],
      "source": [
        "y_pred = text_clf_svm.predict(X_validation)\n",
        "\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "precision = precision_score(y_validation, y_pred)\n",
        "recall = recall_score(y_validation, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4PTqrTxXH8"
      },
      "source": [
        "# Test set performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFZu3xISxMQS",
        "outputId": "723631cb-6cd7-4b9b-ecac-4113b7c31171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.924187725631769\n",
            "Precision: 0.9172113289760349\n",
            "Recall: 0.9439461883408071\n"
          ]
        }
      ],
      "source": [
        "y_pred = text_clf_svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1], dtype=int64)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clf_svm.predict([\"Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43mpipeline\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruthOrNot.sav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(text_clf_svm, \"TruthOrNot.sav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
